---
title: "Intoduction into Adjusted Predictions and Marginal Effects"
author: "Daniel LÃ¼decke"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Intoduction into Adjusted Predictions and Marginal Effects}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r set-options, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  dev = "png",
  fig.width = 7,
  fig.height = 3.5,
  message = FALSE, warning = FALSE)
options(width = 800)

if (!requireNamespace("ggplot2", quietly = TRUE) ||
    !requireNamespace("parameters", quietly = TRUE) ||
    !requireNamespace("margins", quietly = TRUE)) {
  knitr::opts_chunk$set(eval = FALSE)
}
```

# Marginal effects and predictions

There is no common language across fields regarding a unique meaning of "marginal effects". Thus, the wording throughout this package may vary. The most generic description of what *ggeffects* does, is: *ggeffects* allows us to interpret a statistical model by making predictions generated by the model when one holds the non-focal variables constant and varies the focal variable(s).

In the following, some examples are shown to make clear what is actually calculated and returned by the package's functions `ggpredict()`, `ggemmeans()` and `ggeffect()`, and how this differs from other functions or software packages that calculate marginal effects.

## An example with a simple linear model

First, we fit s simple linear model and look at the coefficient of the only predictor, `Sepal.Width`.

```{r}
data(iris)
model1 <- lm(Sepal.Length ~ Sepal.Width, data = iris)
coef(model1)["Sepal.Width"]
```

The coefficient we see here, is in this simple case the _slope_ of the regression line:

```{r}
library(ggplot2)
ggplot(iris, aes(x = Sepal.Width, y = Sepal.Length)) + 
  geom_point() +
  geom_abline(intercept = coef(model1)["(Intercept)"], 
              slope = coef(model1)["Sepal.Width"])
```

In this simple case of a linear model, the slope for this regression line is always the same for each value of our predictor, `Sepal.Width`. We can check this by generating predictions of our model.

```{r}
library(ggeffects)
pr <- ggpredict(model1, "Sepal.Width")
pr
```

"Predictions" returned by *ggeffects* are actually interpretations of regression coefficients in terms of _comparison_. We can compare how much our outcome (`Sepal.Length`) changes on average, when the focal term (in this case: `Sepal.Width`) varies: For example, what is the average value of `Sepal.Length` for observations with a value of, say, `2` for `Sepal.Width`, compared to observations with a `3` for `Sepal.Width`?

*ggeffects* returns predictions for [representative values](https://strengejacke.github.io/ggeffects/articles/introduction_effectsatvalues.html) for the focal term, hence you see many predicted values (including confidence intervals) in the output for the different values of the focal term(s).

If we now look at the differences between any two predicted values, we see that these are identical:

```{r}
# Difference between predicted values for Sepal.Width = 2 and 3
pr <- ggpredict(model1, "Sepal.Width [2,3]")
round(diff(pr$predicted), 4)

# Difference between predicted values for Sepal.Width = 3 and 4
pr <- ggpredict(model1, "Sepal.Width [4,5]")
round(diff(pr$predicted), 4)
```

Furthermore, the difference of predicted values that differ by `1` in the focal term (`Sepal.Width`), equals the regression coefficient. This is because the interpretation of a regression coefficient can be seen as average difference in the outcome, "comparing two individuals that differ in one predictor [a difference of `1` for `Sepal.Width` in this case], while being at the same levels of all other predictors." (Gelman, Hill, Vehtari 2020, page 494). We don't have any other predictors in this example, so we don't go into deeper details here.

Thus, the association - or _effect_ - between `Sepal.Length` and `Sepal.Width` is the same for every value of `Sepal.Width`. This means, that for simple linear models, the regression coefficient is also the _marginal effect_:

```{r}
library(margins)
margins(model1)
```

The _marginal effect_ here is the _average marginal effect_, because on average, the effect of `Sepal.Width` on `Sepal.Length` is `r round(as.vector(coef(model1)["Sepal.Width"]), 4)`.

## An example with a simple logistic regression model

For the next example, we simulate some data for a logistic regression model.

```{r}
set.seed(123)
y <- rbinom(300, 1, c(.3, .7))
x <- rnorm(300, 2)
y_1 <- y == 1
x[y_1] <- x[y_1] + rnorm(sum(y_1), 3)

d <- data.frame(x, y)
model2 <- glm(y ~ x, family = binomial(), data = d)

coef(model2)["x"]
```

The regression coefficient for `x` (on the logit-scale) is `r round(as.vector(coef(model2)["x"]), 4)`. However, for a logistic regression, this "slope" is not constant across all values of `x`, because we have non-linear transformations here. We can make this more clear by looking at the predicted probabilities:

```{r, message=FALSE}
plot(ggpredict(model2, "x [all]"), ci = FALSE, add.data = TRUE)
```

We have some differences here compared to the linear regression model:

1. We no longer have the predicted _average difference_ or _mean_ in our outcome, but rather the predicted _probability_ that our outcome is `1` for a given value of `x`.

2. Due to the non-linear transformation, the slope differs at different values for `x`, thus, the "marginal effect" or "association" (in terms of probabilities) is not constant across values of `x`.

3. While the regression coefficient in linear models is already on the response scale, and hence the (average) marginal effect equals the regression coefficient, we have different scales in logistic regression models: the coefficients shown in `summary()` are on the logit-scale (the scale of the linear predictor); exponentiating that coefficient (i.e. `exp(coef(model2))`) returns an _odds ratio_; predictions are easy to interpret in terms of _probabilities_, as mentioned under 1).

First, let's look at the average marginal effect of `x` in this model:

```{r}
margins(model2)
```

The result indicates "the contribution of each variable on the outcome scale", i.e. the "change in the predicted probability that the outcome equals 1" (see vignettes from the [*margins* package](https://cran.r-project.org/package=margins)). _On average_, a unit-change in `x` changes the predicted probability that the outcome equals 1 by `r insight::format_value(mean(marginal_effects(margins(model2), with_at = FALSE)$dydx_x), as_percent = TRUE)`.

I personally find it less intuitive to interpret _average marginal effects_, but rather prefer to look at predictions at different values of the focal term(s), which is what *ggeffects*  returns by default:

```{r}
ggpredict(model2, "x")
```

For `x = -2`, the predicted probability that `y = 1`, as estimated by our model, is zero. For `x = 10`, the probability is 100\%. In essence, what `ggpredict()` returns, are not _average_ marginal effects, but rather the marginal effects at different values of `x`.

# Marginal effects, average marginal effects or predictions?

Following [these lecture-notes](https://clas.ucdenver.edu/marcelo-perraillon/code-and-topics/marginal-effects), marginal effects are predictions. The main difference is how "effects" is understood. In particular in econometrics, "marginal effects" are understood as predictions for numerical derivates of the focal term, which is achieved by Stata's `margins, dydx(varname)` or R's `margins::dydx()`. However, marginal effects _at specific values_, in Stata `margins, at(var1 = 5, var2 = 10)`, are considered as _predictions_ - and this kind of "marginal effects" is what *ggeffects* returns.

Thus, the language used throughout this package considers _marginal effects_ as predictions, i.e. predicted values. Depending on the response scale, these are either predicted (mean) values, predicted probabilities, predicted (mean) count (for count models) etc. Currently, *ggeffects* does _not_ calculate _average_ marginal effects.

# Estimated marginal means

Sometimes, the term _estimated marginal means_ is used as well, because this is commonly used in software packages likes SPSS, but there is also a prominent R package, [*emmeans*](https://cran.r-project.org/package=emmeans).

But what is the difference, for instance, between simple means and "estimated marginal" means? And why "marginal"? The idea behind marginal effects, and estimated marginal means, is that the estimated (or predicted) average outcome value is adjusted for the remaining co-variates. We shall demonstrate this with two binary variables in a logistic regression model.

We first simulate some fake data:

```{r}
smoking <- data.frame(
  sex = factor(c("male", "female", "female", "male", "female", "female",
                 "male", "female", "female", "male", "male", "female",
                 "female"), 
               levels = c("male", "female")),
  smoking = factor(c("no", "yes", "yes", "yes", "yes", "no", "no", "yes",
                     "yes", "no", "no", "no", "yes"), 
                   levels = c("no", "yes")),
  age = c(10, 45, 50, 40, 45, 12, 14, 55, 60, 10, 14, 50, 40)
)
```

Looking at the proportions of the table, we see that many more female persons are smoking compared to male persons:

```{r}
100 * proportions(table(smoking$sex, smoking$smoking), margin = 1)
```

In this case, we have no "estimated" or "predicted" means or averages, but predicted probabilities. According to the table, the probability of being female and smoking is 75\%, while it's only 20\% for male persons. We get the same values for the predicted probabilities, if we run a logistic regression model:

```{r}
library(parameters)
model3 <- glm(smoking ~ sex, family = binomial(), data = smoking)

# Looking at the odds ratio for "sex"
model_parameters(model3, exponentiate = TRUE)

# Looking at the predicted probabilities for "sex"
ggpredict(model3, "sex")
```

The reference category for `sex` is _`r levels(smoking$sex)[1]`_, so we can estimate the average marginal effects for female persons using `margins()`:

```{r}
margins(model3)
```

The interpretation is like stated above: the change in the predicted probability that the outcome equals 1 for female persons is 0.55, i.e. 55\%. This is exactly the difference between the predicted probabilities for male and female persons.

Looking at the age distribution in the sample, we might conclude that our model produces biased estimates, and therefor biased predictions. Remember the high odds ratio of our model, as shown above. Now we include `age` as possible confounder in our model.

```{r}
model4 <- glm(smoking ~ sex + age, family = binomial(), data = smoking)

# Looking at the odds ratio for "sex"
model_parameters(model4, exponentiate = TRUE)

# Looking at the predicted probabilities for "sex"
ggpredict(model4, "sex")
```


# References

Gelman A, Hill J, Vehtari A (2020): "Regression and Other Stories". Cambridge.
